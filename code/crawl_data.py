import os
import json
import random
from time import sleep
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ========== C·∫§U H√åNH ==========

script_dir = os.path.dirname(os.path.abspath(__file__))
chromedriver_path = os.path.join(script_dir, "..", "set_up", "chromedriver-win64", "chromedriver.exe")
news_data_path = os.path.join(script_dir, "..", "data", "news_data.json")

os.makedirs(os.path.dirname(news_data_path), exist_ok=True)

domain = "https://vtv.vn/"
categories = [
    "chinh-tri", "xa-hoi", "phap-luat", "the-gioi", "kinh-te", "the-thao",
    "truyen-hinh", "giai-tri", "suc-khoe", "doi-song", "cong-nghe", "giao-duc"
]

# ========== H·ªñ TR·ª¢ ==========

def click_load_more(driver, max_clicks=20):
    """Click v√†o n√∫t 'Xem th√™m' t·ªëi ƒëa max_clicks l·∫ßn v√† ch·ªù b√†i m·ªõi load ra"""
    try:
        old_articles = driver.find_elements(By.CSS_SELECTOR, 'a[data-linktype="newsdetail"]')
        old_count = len(old_articles)
        clicks = 0

        while clicks < max_clicks:
            load_more_button = driver.find_element(By.CSS_SELECTOR, "div.loadmore a")
            driver.execute_script("arguments[0].click();", load_more_button)
            print(f"   üîÑ ƒê√£ click 'Xem th√™m' l·∫ßn {clicks + 1}...")
            clicks += 1

            # Ch·ªù b√†i m·ªõi load ra
            WebDriverWait(driver, 10).until(
                lambda d: len(d.find_elements(By.CSS_SELECTOR, 'a[data-linktype="newsdetail"]')) > old_count
            )
            sleep(random.uniform(1.5, 3.5))
            
            # Ki·ªÉm tra n·∫øu kh√¥ng c√≤n n√∫t "Xem th√™m" th√¨ d·ª´ng
            if len(driver.find_elements(By.CSS_SELECTOR, "div.loadmore a")) == 0:
                break

        return True

    except Exception as e:
        print(f"   ‚ö†Ô∏è L·ªói khi click 'Xem th√™m': {e}")
        return False

def extract_article_data(driver, link, title, category):
    """Truy c·∫≠p t·ª´ng b√†i v√† l·∫•y d·ªØ li·ªáu chi ti·∫øt"""
    try:
        driver.get(link)
        sleep(random.uniform(1, 2))

        try:
            author_text = driver.find_element(By.CSS_SELECTOR, 'p.author').text
            author = author_text.split("-")[0].strip()
        except:
            author = ""

        try:
            short_description = driver.find_element(By.CSS_SELECTOR, 'h2.sapo').text[9:]
        except:
            short_description = ""

        content = ""
        try:
            paragraphs = driver.find_element(By.ID, 'entry-body').find_elements(By.TAG_NAME, 'p')
            for p in paragraphs:
                text = p.text.strip()
                if not text.startswith("* M·ªùi qu√Ω ƒë·ªôc gi·∫£"):
                    content += text + " "
            content = content.strip()
        except:
            content = ""

        return {
            "title": title,
            "category": category,
            "news_link": link,
            "author": author,
            "short_description": short_description,
            "content": content
        }
    except Exception as e:
        print(f"   ‚ùå L·ªói khi l·∫•y d·ªØ li·ªáu b√†i {link}: {e}")
        return None

# ========== KH·ªûI T·∫†O DRIVER ==========

options = Options()
# options.add_argument("--headless")  # N·∫øu mu·ªën ch·∫°y ·∫©n tr√¨nh duy·ªát
driver = webdriver.Chrome(service=Service(chromedriver_path), options=options)

# ========== CRAWLING ==========

news_links = []
all_data = []

for category in categories:  # Crawl th·ª≠ 2 category ƒë·∫ßu ti√™n
    print(f"\n‚ñ∂ Ch·ªß ƒë·ªÅ: {category}")
    category_url = f"{domain}{category}.htm"
    driver.get(category_url)
    sleep(random.uniform(1, 2))

    # ·∫§n "Xem th√™m" t·ªëi ƒëa 20 l·∫ßn
    if not click_load_more(driver, max_clicks=20):
        print("   ‚ö†Ô∏è Kh√¥ng th·ªÉ ti·∫øp t·ª•c click 'Xem th√™m'. D·ª´ng l·∫°i.")
        break

    # L·∫•y danh s√°ch b√†i vi·∫øt sau khi load h·∫øt
    article_elements = driver.find_elements(By.CSS_SELECTOR, 'a[data-linktype="newsdetail"]')
    print(f"   üîé T·ªïng s·ªë b√†i t√¨m th·∫•y: {len(article_elements)}")

    for article in article_elements:
        try:
            link = article.get_attribute("href")
            title = article.get_attribute("title")
            if link and (link, title) not in news_links:
                news_links.append((link, title, category))
        except Exception as e:
            print(f"   ‚ö†Ô∏è L·ªói ƒë·ªçc link: {e}")
            continue

# Crawl chi ti·∫øt t·ª´ng b√†i vi·∫øt
print("\n‚ñ∂ B·∫Øt ƒë·∫ßu v√†o t·ª´ng b√†i vi·∫øt ƒë·ªÉ l·∫•y th√¥ng tin...")
for link, title, category in news_links:
    print(f"‚Üí ƒêang l·∫•y: {title}")
    article_data = extract_article_data(driver, link, title, category)
    if article_data:
        all_data.append(article_data)
    sleep(random.uniform(1, 2))

# ========== L∆ØU FILE ==========

with open(news_data_path, "w", encoding="utf-8") as f:
    json.dump(all_data, f, ensure_ascii=False, indent=4)

print(f"\n‚úÖ ƒê√£ l∆∞u {len(all_data)} b√†i vi·∫øt v√†o {news_data_path}")

driver.quit()
